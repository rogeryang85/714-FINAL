{"cells":[{"cell_type":"markdown","id":"96b7476a","metadata":{"id":"96b7476a"},"source":["# 10-714 Homework 4 Extension"]},{"cell_type":"markdown","id":"5a89d459","metadata":{"id":"5a89d459"},"source":["This homework is an extension of homework 4, where you will be implementing the Transformer architecture. For this assignment, all the things you need to implement is in the file `python/needle/nn/nn_transformer.py`. Other things in the needle library remains the same. This homework extension is built on homework 4, so make sure to copy the solutions from homework 4."]},{"cell_type":"code","execution_count":1,"id":"c1a5c18f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1a5c18f","executionInfo":{"status":"ok","timestamp":1733869314539,"user_tz":300,"elapsed":19551,"user":{"displayName":"Roger Yang","userId":"14646738617737381987"}},"outputId":"fc12c7e4-7879-40a2-8dc8-ab3d37369318"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting git+https://github.com/dlsys10714/mugrade.git\n","  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-7igyfrj4\n","  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-7igyfrj4\n","  Resolved https://github.com/dlsys10714/mugrade.git to commit 656cdc2b7ad5a37e7a5347a7b0405df0acd72380\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: mugrade\n","  Building wheel for mugrade (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mugrade: filename=mugrade-1.2-py3-none-any.whl size=3935 sha256=5968b458e0efc297e5ebabad0bca01f1cc268177a28b68d5fa8f0fab3b771cb3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-4hk5pbdw/wheels/8b/ba/3a/621da1207eab160c01968c5e0bd1266f505b9e3f8010376d61\n","Successfully built mugrade\n","Installing collected packages: mugrade\n","Successfully installed mugrade-1.2\n","Collecting pybind11\n","  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pybind11\n","Successfully installed pybind11-2.13.6\n"]}],"source":["# Code to set up the assignment\n","from google.colab import drive\n","drive.mount('/content/drive')\n","#%cd /content/drive/MyDrive/\n","#!mkdir -p 10714\n","#%cd /content/drive/MyDrive/10714\n","#!git clone https://github.com/dlsyscourse/hw4_extra.git\n","#%cd \"/content/drive/Shared with me/hw4_extra\"\n","\n","!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n","!pip3 install pybind11"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"UtGidD8GNsYw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733869666853,"user_tz":300,"elapsed":664,"user":{"displayName":"Roger Yang","userId":"14646738617737381987"}},"outputId":"b41c2940-9160-42d6-8cd9-14cf315e71eb"},"id":"UtGidD8GNsYw","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/10714/hw4_extra"],"metadata":{"id":"mbA7KtzCOTC-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733869725229,"user_tz":300,"elapsed":67,"user":{"displayName":"Roger Yang","userId":"14646738617737381987"}},"outputId":"c573615b-6f52-46aa-b915-caccae6945cc"},"id":"mbA7KtzCOTC-","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/10714/hw4_extra\n"]}]},{"cell_type":"code","execution_count":7,"id":"5c9fb467","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5c9fb467","executionInfo":{"status":"ok","timestamp":1733869766319,"user_tz":300,"elapsed":36474,"user":{"displayName":"Roger Yang","userId":"14646738617737381987"}},"outputId":"e4f0221d-fafa-40b8-e6b3-bbecd4c210ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["-- Found pybind11: /usr/local/lib/python3.10/dist-packages/pybind11/include (found version \"2.13.6\")\n","-- Found cuda, building cuda backend\n","Tue Dec 10 22:28:51 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   29C    P0              43W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n","-- Configuring done (1.9s)\n","-- Generating done (4.3s)\n","-- Build files have been written to: /content/drive/MyDrive/10714/hw4_extra/build\n","make[1]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","make[2]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","make[3]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","make[3]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","[-25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n","[  0%] \u001b[32m\u001b[1mLinking CXX shared module /content/drive/MyDrive/10714/hw4_extra/python/needle/backend_ndarray/ndarray_backend_cpu.cpython-310-x86_64-linux-gnu.so\u001b[0m\n","make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","[  0%] Built target ndarray_backend_cpu\n","make[3]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","[ 25%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n","make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","make[3]: Entering directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","[ 50%] \u001b[32m\u001b[1mLinking CXX shared module /content/drive/MyDrive/10714/hw4_extra/python/needle/backend_ndarray/ndarray_backend_cuda.cpython-310-x86_64-linux-gnu.so\u001b[0m\n","make[3]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","[ 50%] Built target ndarray_backend_cuda\n","make[2]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n","make[1]: Leaving directory '/content/drive/MyDrive/10714/hw4_extra/build'\n"]}],"source":["!make"]},{"cell_type":"code","execution_count":null,"id":"45349235","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45349235","executionInfo":{"status":"ok","timestamp":1733416688020,"user_tz":300,"elapsed":12,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"613e92a1-4ad2-48d7-fd9a-3bfd1fd328b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: PYTHONPATH=./python\n","env: NEEDLE_BACKEND=nd\n"]}],"source":["%set_env PYTHONPATH ./python\n","%set_env NEEDLE_BACKEND nd"]},{"cell_type":"code","execution_count":null,"id":"f54d7073","metadata":{"id":"f54d7073"},"outputs":[],"source":["import sys\n","sys.path.append('./python')"]},{"cell_type":"code","execution_count":null,"id":"c5945207","metadata":{"id":"c5945207"},"outputs":[],"source":["# Download the PTB dataset\n","\n","import urllib.request\n","import os\n","\n","!mkdir -p './data/ptb'\n","# Download Penn Treebank dataset\n","ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n","for f in ['train.txt', 'test.txt', 'valid.txt']:\n","    if not os.path.exists(os.path.join('./data/ptb', f)):\n","        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))"]},{"cell_type":"markdown","id":"1cea5c0a","metadata":{"id":"1cea5c0a"},"source":["## Transformers"]},{"cell_type":"markdown","id":"68a2f639","metadata":{"id":"68a2f639"},"source":["In the previous homework you have implemented two sequence models, the Recurrent Neural Network, and Long Short-Term Memory. These models were once the state-of-the-art and default architecture choices on sequence modelling tasks, including language generation, until recently when the famous paper \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\" (Vaswani et al. 2017) came out in 2017. Since then, Transformers, a model architecture introduced in the aforementioned paper, have become the standard and most performant class of model on language tasks.\n","\n","You will be implementing a Transformer in `python/needle/nn/nn_transformer.py`.\n","\n","Transformers are composed of three mains components that you will implement.\n","1. A masked multi-head attention mechanism that adaptively focuses on different timesteps of a sequence.\n","2. A residual block consisting of the attention layer followed by a two-layer neural network applied independently at each timestep.\n","3. A Transformer model consisting of several stacked residual blocks (in this homework you will implement a decoder-only transformer).\n","\n","![model](https://miro.medium.com/v2/1*ZCFSvkKtppgew3cc7BIaug.png)\n","\n","The above is a photo of the Transformer architecture from Vaswani et al. 2017. The version of the transformer you will implement is nearly identical, but has layer normalization applied at the start of each residual block (referred to as a [prenorm variant](https://arxiv.org/abs/2002.04745) of the Transformer)."]},{"cell_type":"markdown","id":"f094ff30","metadata":{"id":"f094ff30"},"source":["## Part 1: Implementing the Multi-Head Attention Activation Layer\n","\n","In this subproblem, you will be implementing the `forward` function of a \"base\" attention activation layer `MultiHeadAttention` in `python/needle/nn/nn_transformer.py`. This activation layer will take in three inputs:\n","<p style=\"text-align: center;\">multi-head queries $Q \\in R^\\mathcal{B \\times H \\times T \\times D}$, keys $K \\in R^\\mathcal{B \\times H \\times T \\times D}$, and values $V \\in R^\\mathcal{B \\times H \\times T \\times D}$</p>\n","\n","where $B$ is the batch size, $H$ is the number of attention heads, $T$ is the sequence length, and $D$ is the hidden dimension.\n","\n","The attention output $X \\in R^{B \\times H \\times T \\times D}$ is computed as follows:\n","\n","<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q K^T}{\\sqrt{D}}) V$</p>\n","\n","Note that the matrix multiplications above are batched. This functionality is not natively supported in needle yet, so we have provided a convenient function `matmul` for batched matrix multiplications in `MultiHeadAttention`. Your goal in this section is to return $X$ given the input queries, keys, and values.\n","\n","For auto-regressive Transformer, this attention should support causal masking using the function `self.create_causal_mask` we have provided. This is to make sure that the prediction of next token only depends on it's previous tokens. Specifically, causal masking is applying a mask before the softmax so that the softmax probability is computed over a masked matrix of $\\frac{Q K^T}{\\sqrt{D}}$.\n","\n","In addition, your implementation should apply dropout to the attention softmax $\\text{softmax}(\\frac{Q K^T}{\\sqrt{D}})$. You can use the `self.dropout` function of the `MultiHeadAttention` module.\n","\n","Importantly, this layer is only an activation function, and has no trainable variables (these come later).\n","\n","Once you have finished your implementation, test your code with the following test cases."]},{"cell_type":"code","execution_count":null,"id":"df7eeaa9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"df7eeaa9","executionInfo":{"status":"ok","timestamp":1731705374886,"user_tz":300,"elapsed":27124,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"ed200f19-328d-4c5a-8ec4-07751e40ce4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/10714/hw4_extra\n","plugins: typeguard-4.4.1, anyio-3.7.1\n","collected 112 items / 96 deselected / 16 selected                                                  \u001b[0m\n","\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.0-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cpu-0.1-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 56%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 68%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.0-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-False-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 93%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_activation[cuda-0.1-True-64-31-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n","\n","\u001b[32m================================ \u001b[32m\u001b[1m16 passed\u001b[0m, \u001b[33m96 deselected\u001b[0m\u001b[32m in 23.39s\u001b[0m\u001b[32m ================================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"attention_activation\""]},{"cell_type":"code","execution_count":null,"id":"d19da8e2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d19da8e2","executionInfo":{"status":"ok","timestamp":1731705392589,"user_tz":300,"elapsed":16941,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"dcfe00d9-bbee-4200-a0d5-54a17433085e"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0\n","rootdir: /content/drive/MyDrive/10714/hw4_extra\n","plugins: typeguard-4.4.1, anyio-3.7.1\n","\u001b[1mcollecting ... \u001b[0mUsing needle backend\n","collected 4 items / 3 deselected / 1 selected                                                      \u001b[0m\n","\n","tests/hw4_extra/test_transformer.py \n","Submitting attention_activation...\n","(4, 5, 31, 31) (4, 5, 31, 64)\n","Grader test 1 passed\n","(4, 5, 31, 31) (4, 5, 31, 64)\n","Grader test 2 passed\n","(4, 5, 31, 31) (4, 5, 31, 64)\n","Grader test 3 passed\n","(4, 5, 31, 31) (4, 5, 31, 64)\n","Grader test 4 passed\n","(4, 5, 31, 31) (4, 5, 31, 64)\n","Grader test 5 passed\n","(4, 5, 31, 31) (4, 5, 31, 64)\n","Grader test 6 passed\n","(4, 5, 31, 31) (4, 5, 31, 64)\n","Grader test 7 passed\n","(4, 5, 31, 31) (4, 5, 31, 64)\n","Grader test 8 passed\n","(8, 5, 31, 31) (8, 5, 31, 64)\n","Grader test 9 passed\n","(8, 5, 31, 31) (8, 5, 31, 64)\n","Grader test 10 passed\n","(8, 5, 31, 31) (8, 5, 31, 64)\n","Grader test 11 passed\n","(8, 5, 31, 31) (8, 5, 31, 64)\n","Grader test 12 passed\n","(8, 5, 31, 31) (8, 5, 31, 64)\n","Grader test 13 passed\n","(8, 5, 31, 31) (8, 5, 31, 64)\n","Grader test 14 passed\n","(8, 5, 31, 31) (8, 5, 31, 64)\n","Grader test 15 passed\n","(8, 5, 31, 31) (8, 5, 31, 64)\n","Grader test 16 passed\n","\u001b[32m.\u001b[0m\n","\n","\u001b[32m================================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m3 deselected\u001b[0m\u001b[32m in 15.47s\u001b[0m\u001b[32m =================================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"cpXlcDbbqL5zz45diS6Z\" -k \"attention_activation\""]},{"cell_type":"markdown","id":"0e65aea6","metadata":{"id":"0e65aea6"},"source":["## Part 2 Implementing the Self-Attention Layer with trainable parameters\n","\n","In this subproblem, you will use the `MultiHeadAttention` class you just implemented, and wrap it in a subclass of `Module` called `AttentionLayer` in `python/needle/nn/nn_transformer.py`.\n","\n","This layer implements the self-attention with prenorm (when k, and v are None in the `self.forward` call) and cross-attention (when k and v are present in the `self.forward` call). We have provided skeleton code with the appropriate layer attributes defined. Your job is to write the forward pass of the `AttentionLayer`. Note that you are implementing multi-head attention, where the number of attention heads is given by the `self.num_head` attribute of the `AttentionLayer` class.\n","\n","Given inputs $Q \\in R^\\mathcal{B \\times T \\times D'}$, keys $K \\in R^\\mathcal{B \\times T \\times D'}$, and values $V \\in R^\\mathcal{B \\times T \\times D'}$ where $B$ is the batch size, $T$ is the sequence length, and $D'$ is the embedding dimension. This layer performs the following computation sequentially:\n","\n","(1) map queries, key, and values to heads.\n","\n","<p style=\"text-align: center;\">$Q' = \\text{LayerNorm}_q (Q) \\; W_q$</p>\n","\n","<p style=\"text-align: center;\">$K' = \\text{LayerNorm}_k (K) \\; W_k$</p>\n","\n","<p style=\"text-align: center;\">$V' = \\text{LayerNorm}_v (V) \\; W_v$</p>\n","\n","where $\\text{LayerNorm}_q , \\text{LayerNorm}_k, \\text{LayerNorm}_v $ are the prenorm `self.prenorm_q`, `self.prenorm_k` and `self.prenorm_v` respectively.\n","\n","(2) unravel heads from the channels axis.\n","\n","<p style=\"text-align: center;\">$Q' \\in R^{B \\times T \\times (HD)} \\to Q' \\in R^{B \\times H \\times T \\times D} $</p>\n","\n","<p style=\"text-align: center;\">$K' \\in R^{B \\times T \\times (HD)} \\to K' \\in R^{B \\times H \\times T \\times D} $</p>\n","\n","<p style=\"text-align: center;\">$V' \\in R^{B \\times T \\times (HD)} \\to V' \\in R^{B \\times H \\times T \\times D} $</p>\n","\n","where $H$ and $D$ are `self.num_head` and `self.head_dim` respectively.\n","\n","(3) compute the multi-head attention activation.\n","\n","<p style=\"text-align: center;\">$X = \\text{softmax}(\\frac{Q' (K')^T}{\\sqrt{D}}) V'$</p>\n","\n","<p style=\"text-align: center;\">$X \\in R^{B \\times H \\times T \\times D} \\to X \\in R^{B \\times T \\times H \\times D} $</p>\n","\n","<p style=\"text-align: center;\">$X \\in R^{B \\times T \\times H \\times D} \\to X \\in R^{B \\times T \\times (HD)}$</p>\n","\n","The last two steps do a transpose and then reshape to get the hidden states to be the correct shape.\n","\n","(4) project back to the input space of the layer with `self.out_projection`\n","\n","<p style=\"text-align: center;\">$X' = X \\; W_o$</p>\n","\n","Your goal in this part is to return $X$ in the `self.forward` call of `AttentionLayer`. For debugging, you may capture the `probs` variable returned by the inner `MultiHeadAttention` module and store it in an attribute such as `self.probs` of the attention layer.\n","\n","Once finished, you may test your layer with the following test cases."]},{"cell_type":"code","execution_count":null,"id":"44b2fe04","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44b2fe04","executionInfo":{"status":"ok","timestamp":1731707573578,"user_tz":300,"elapsed":7159,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"d5aeda6c-d9fe-4df1-cebc-94a2ec388f95"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/10714/hw4_extra\n","plugins: typeguard-4.4.1, anyio-3.7.1\n","collected 112 items / 80 deselected / 32 selected                                                  \u001b[0m\n","\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m  [  3%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m  [  6%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 15%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 18%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 21%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.0-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 25%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 28%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 31%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-False-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 40%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m   [ 43%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 46%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cpu-0.1-True-32-8-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m  [ 50%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-False-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 65%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 68%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.0-True-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-False-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 90%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 93%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_attention_layer[cuda-0.1-True-32-8-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","============================================= FAILURES =============================================\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.0-False-32-8-27-5-4] _________________________\u001b[0m\n","\n","batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, causal = False\n","dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","input_dim  = 27\n","k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n","         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n","          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n","num_head   = 8\n","q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","seq_len    = 5\n","v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n","         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n","         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = False\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.0\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af976399ab0>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af9763999c0>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af976399f60>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.0-False-32-8-27-5-8] _________________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, causal = False\n","dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","input_dim  = 27\n","k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n","         -0.37128997,  0.06527077],\n","        [-1.0428...\n","        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n","          0.526678  ,  0.26005384]]], dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","seq_len    = 5\n","v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n","         -0.7493343 , -0.15542848],\n","        [ 2.5435...\n","        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n","          0.08334691,  0.17072625]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = False\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.0\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af9761e50f0>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af9761e51e0>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af9761e53f0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m________________________ test_attention_layer[cuda-0.0-False-32-8-27-11-4] _________________________\u001b[0m\n","\n","batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, causal = False\n","dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","input_dim  = 27\n","k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n","         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n","          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n","      dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","seq_len    = 11\n","v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n","          1.8853118 , -0.61863774],\n","        [-0.3089...\n","        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n","          0.8674379 ,  1.4639174 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = False\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.0\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af9762855a0>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af976285660>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af976285570>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m________________________ test_attention_layer[cuda-0.0-False-32-8-27-11-8] _________________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, causal = False\n","dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","input_dim  = 27\n","k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n","          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n","         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n","      dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","seq_len    = 11\n","v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n","          0.3303766 , -0.71756196],\n","        [-1.2459...\n","        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n","          1.0845269 ,  0.7676009 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = False\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.0\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af976467b20>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af976466650>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af976467c70>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.0-True-32-8-27-5-4] __________________________\u001b[0m\n","\n","batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, causal = True\n","dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","input_dim  = 27\n","k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n","         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n","          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n","num_head   = 8\n","q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","seq_len    = 5\n","v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n","         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n","         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = True\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.0\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af976399fc0>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af97639a110>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af976399840>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.0-True-32-8-27-5-8] __________________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, causal = True\n","dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","input_dim  = 27\n","k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n","         -0.37128997,  0.06527077],\n","        [-1.0428...\n","        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n","          0.526678  ,  0.26005384]]], dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","seq_len    = 5\n","v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n","         -0.7493343 , -0.15542848],\n","        [ 2.5435...\n","        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n","          0.08334691,  0.17072625]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = True\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.0\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af9764509a0>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af976453d90>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af9764508b0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.0-True-32-8-27-11-4] _________________________\u001b[0m\n","\n","batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, causal = True\n","dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","input_dim  = 27\n","k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n","         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n","          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n","      dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","seq_len    = 11\n","v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n","          1.8853118 , -0.61863774],\n","        [-0.3089...\n","        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n","          0.8674379 ,  1.4639174 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = True\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.0\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af97645bfd0>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af9764599c0>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af9764598d0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.0-True-32-8-27-11-8] _________________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, causal = True\n","dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","input_dim  = 27\n","k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n","          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n","         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n","      dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","seq_len    = 11\n","v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n","          0.3303766 , -0.71756196],\n","        [-1.2459...\n","        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n","          1.0845269 ,  0.7676009 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = True\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.0\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af976466020>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af9764665c0>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af976467790>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.1-False-32-8-27-5-4] _________________________\u001b[0m\n","\n","batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, causal = False\n","dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","input_dim  = 27\n","k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n","         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n","          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n","num_head   = 8\n","q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","seq_len    = 5\n","v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n","         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n","         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = False\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.1\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af976261990>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af976261240>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af976263940>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.1-False-32-8-27-5-8] _________________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, causal = False\n","dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","input_dim  = 27\n","k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n","         -0.37128997,  0.06527077],\n","        [-1.0428...\n","        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n","          0.526678  ,  0.26005384]]], dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","seq_len    = 5\n","v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n","         -0.7493343 , -0.15542848],\n","        [ 2.5435...\n","        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n","          0.08334691,  0.17072625]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = False\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.1\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af9762237f0>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af976222260>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af976223490>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m________________________ test_attention_layer[cuda-0.1-False-32-8-27-11-4] _________________________\u001b[0m\n","\n","batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, causal = False\n","dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","input_dim  = 27\n","k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n","         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n","          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n","      dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","seq_len    = 11\n","v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n","          1.8853118 , -0.61863774],\n","        [-0.3089...\n","        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n","          0.8674379 ,  1.4639174 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = False\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.1\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af9761fbbb0>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af9761fb610>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af9761fad10>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m________________________ test_attention_layer[cuda-0.1-False-32-8-27-11-8] _________________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, causal = False\n","dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","input_dim  = 27\n","k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n","          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n","         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n","      dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","seq_len    = 11\n","v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n","          0.3303766 , -0.71756196],\n","        [-1.2459...\n","        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n","          1.0845269 ,  0.7676009 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = False\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.1\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af976399f60>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af976399810>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af976399a20>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.1-True-32-8-27-5-4] __________________________\u001b[0m\n","\n","batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, causal = True\n","dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","input_dim  = 27\n","k          = array([[[-0.17591418, -0.81691647, -0.38043165, -1.4896185 ,\n","         -0.3424145 ,  0.5975326 , -0.55117905, -0.862665...  -0.12426752,  1.3688309 , -0.28665462,  1.8718244 ,\n","          1.2692264 ,  1.2295368 , -0.991682  ]]], dtype=float32)\n","num_head   = 8\n","q          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","seq_len    = 5\n","v          = array([[[-1.32029665e+00,  3.13428760e-01,  3.91371369e-01,\n","         -1.15919903e-01, -1.81199983e-01,  1.58794081e+00...,  6.39540374e-01,  1.90380514e-01,\n","         -7.48570561e-02,  2.28314137e+00, -8.06154013e-01]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = True\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.1\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af97645bc70>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af976459db0>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af97645b8b0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.1-True-32-8-27-5-8] __________________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, causal = True\n","dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","input_dim  = 27\n","k          = array([[[-1.3202966 ,  0.31342876,  0.39137137, ..., -0.05047274,\n","         -0.37128997,  0.06527077],\n","        [-1.0428...\n","        [ 0.7475417 , -0.12896587, -0.21562193, ..., -0.4836035 ,\n","          0.526678  ,  0.26005384]]], dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","seq_len    = 5\n","v          = array([[[-0.05705854, -1.8222909 ,  0.37688625, ..., -0.30623534,\n","         -0.7493343 , -0.15542848],\n","        [ 2.5435...\n","        [-0.59417653, -0.05847015, -0.01491287, ...,  0.67793965,\n","          0.08334691,  0.17072625]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = True\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.1\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af976220a30>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af9762217b0>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af9762200d0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.1-True-32-8-27-11-4] _________________________\u001b[0m\n","\n","batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, causal = True\n","dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","input_dim  = 27\n","k          = array([[[ 3.31167579e-01, -1.21368361e+00, -1.94302607e+00, ...,\n","         -3.49218071e-01, -9.10320699e-01,  2.0637938...49814832e-01,  1.62794697e+00, ...,\n","          1.30049407e-01,  1.38784778e+00,  7.90922821e-01]]],\n","      dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","seq_len    = 11\n","v          = array([[[ 0.46454597, -0.29647216, -0.00637594, ...,  0.8831419 ,\n","          1.8853118 , -0.61863774],\n","        [-0.3089...\n","        [-0.10503879, -1.4589338 , -0.06638545, ..., -0.8672699 ,\n","          0.8674379 ,  1.4639174 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = True\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.1\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af9761ddea0>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af9761dde40>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af9761ddfc0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_________________________ test_attention_layer[cuda-0.1-True-32-8-27-11-8] _________________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, causal = True\n","dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_attention_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        q = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        k = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        v = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       layer = nn.AttentionLayer(\u001b[90m\u001b[39;49;00m\n","            input_dim, num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            dropout=dropout, causal=causal, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","input_dim  = 27\n","k          = array([[[ 4.64545965e-01, -2.96472162e-01, -6.37594005e-03, ...,\n","          8.83141875e-01,  1.88531184e+00, -6.1863774...70587523e-02, -1.71661545e-02, ...,\n","         -1.40956357e-01, -1.46573985e+00, -9.60531652e-01]]],\n","      dtype=float32)\n","num_head   = 8\n","q          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","seq_len    = 11\n","v          = array([[[-0.18071565, -0.93578744, -0.1328542 , ...,  0.21799618,\n","          0.3303766 , -0.71756196],\n","        [-1.2459...\n","        [ 0.25303677,  0.72302336,  0.13070038, ..., -0.46647197,\n","          1.0845269 ,  0.7676009 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:89: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/nn/nn_transformer.py\u001b[0m:192: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.prenorm_q = LayerNorm1d(\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_transformer.AttentionLayer'>\n","        causal     = True\n","        device     = cuda()\n","        dim_head   = 32\n","        dropout    = 0.1\n","        dtype      = 'float32'\n","        k_features = 27\n","        num_head   = 8\n","        out_features = 27\n","        q_features = 27\n","        self       = <needle.nn.nn_transformer.AttentionLayer object at 0x7af9763a6860>\n","        v_features = 27\n","\u001b[1m\u001b[31mpython/needle/nn/nn_basic.py\u001b[0m:235: in __init__\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.weight = Parameter(init.init_initializers.ones(\u001b[94m1\u001b[39;49;00m, \u001b[96mself\u001b[39;49;00m.dim, device=device, dtype=dtype, requires_grad=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        __class__  = <class 'needle.nn.nn_basic.LayerNorm1d'>\n","        device     = cuda()\n","        dim        = 27\n","        dtype      = 'float32'\n","        eps        = 1e-05\n","        self       = <needle.nn.nn_basic.LayerNorm1d object at 0x7af9763a6770>\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:31: in ones\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m constant(\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/init/init_basic.py\u001b[0m:26: in constant\n","    \u001b[0marray = device.full(shape, c, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        c          = 1.0\n","        device     = cuda()\n","        dtype      = 'float32'\n","        requires_grad = True\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:54: in full\n","    \u001b[0marr = \u001b[96mself\u001b[39;49;00m.empty(shape, dtype)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        fill_value = 1.0\n","        self       = cuda()\n","        shape      = (1, 27)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:49: in empty\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray.make(shape, device=\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dtype      = 'float32'\n","        self       = cuda()\n","        shape      = (1, 27)\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (1, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7af9763a6650>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (1, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-5-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-11-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-False-32-8-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-5-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-11-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.0-True-32-8-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-5-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-11-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-False-32-8-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-5-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-11-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_attention_layer[cuda-0.1-True-32-8-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31m=========================== \u001b[31m\u001b[1m16 failed\u001b[0m, \u001b[32m16 passed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 5.77s\u001b[0m\u001b[31m ===========================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"attention_layer\""]},{"cell_type":"code","execution_count":null,"id":"20d0bfad","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20d0bfad","executionInfo":{"status":"ok","timestamp":1731706585592,"user_tz":300,"elapsed":28874,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"81a2633f-1196-4fa3-b565-aedbdd362522"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0\n","rootdir: /content/drive/MyDrive/10714/hw4_extra\n","plugins: typeguard-4.4.1, anyio-3.7.1\n","\u001b[1mcollecting ... \u001b[0mUsing needle backend\n","collected 4 items / 3 deselected / 1 selected                                                      \u001b[0m\n","\n","tests/hw4_extra/test_transformer.py \n","Submitting attention_layer...\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Linear (20, 256) (256, 27)\n","Grader test 1 passed\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Linear (20, 256) (256, 27)\n","Grader test 2 passed\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Linear (20, 256) (256, 27)\n","Grader test 3 passed\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Linear (20, 256) (256, 27)\n","Grader test 4 passed\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Linear (20, 256) (256, 27)\n","Grader test 5 passed\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Linear (20, 256) (256, 27)\n","Grader test 6 passed\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Linear (20, 256) (256, 27)\n","Grader test 7 passed\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","Linear (20, 27) (27, 256)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Linear (20, 256) (256, 27)\n","Grader test 8 passed\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Linear (44, 256) (256, 27)\n","Grader test 9 passed\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Linear (44, 256) (256, 27)\n","Grader test 10 passed\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Linear (44, 256) (256, 27)\n","Grader test 11 passed\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Linear (44, 256) (256, 27)\n","Grader test 12 passed\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Linear (44, 256) (256, 27)\n","Grader test 13 passed\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Linear (44, 256) (256, 27)\n","Grader test 14 passed\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Linear (44, 256) (256, 27)\n","Grader test 15 passed\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","Linear (44, 27) (27, 256)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Linear (44, 256) (256, 27)\n","Grader test 16 passed\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Linear (40, 256) (256, 27)\n","Grader test 17 passed\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Linear (40, 256) (256, 27)\n","Grader test 18 passed\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Linear (40, 256) (256, 27)\n","Grader test 19 passed\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Linear (40, 256) (256, 27)\n","Grader test 20 passed\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Linear (40, 256) (256, 27)\n","Grader test 21 passed\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Linear (40, 256) (256, 27)\n","Grader test 22 passed\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Linear (40, 256) (256, 27)\n","Grader test 23 passed\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","Linear (40, 27) (27, 256)\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Linear (40, 256) (256, 27)\n","Grader test 24 passed\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Linear (88, 256) (256, 27)\n","Grader test 25 passed\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Linear (88, 256) (256, 27)\n","Grader test 26 passed\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Linear (88, 256) (256, 27)\n","Grader test 27 passed\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Linear (88, 256) (256, 27)\n","Grader test 28 passed\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Linear (88, 256) (256, 27)\n","Grader test 29 passed\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Linear (88, 256) (256, 27)\n","Grader test 30 passed\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Linear (88, 256) (256, 27)\n","Grader test 31 passed\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","Linear (88, 27) (27, 256)\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Linear (88, 256) (256, 27)\n","Grader test 32 passed\n","\u001b[32m.\u001b[0m\n","\n","\u001b[32m================================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m3 deselected\u001b[0m\u001b[32m in 27.85s\u001b[0m\u001b[32m =================================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"cpXlcDbbqL5zz45diS6Z\" -k \"attention_layer\""]},{"cell_type":"markdown","id":"9fa8fb30","metadata":{"id":"9fa8fb30"},"source":["## Part 3 Implementing a prenorm residual Transformer Layer\n","\n","You now have all the parts necessary to build a full Transformer by this point. In this subproblem, you will assemble the attention layer with a feedforward network into a stackable residual block. We have provided starter code in the `TransformerLayer` class.\n","\n","You will need to define the necessary class attributes in the `self.__init__` call of the module `TransformerLayer`, and fill in the forward pass in `self.forward`. Your transformer layer should support dropout applied to $X'$ from the previous step before adding a residual connection. Implement the following pseudocode of the layer, properly handling the intermediate tensor shapes:\n","\n","x - current sequence of hidden states\n","\n","<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Attention}(x))$</p>\n","<p style=\"text-align: center;\">$x = x + \\text{Dropout}(\\text{Linear}_{2}(\\text{Dropout}(\\text{ReLU}(\\text{Linear}_{1}(\\text{LayerNorm1d}(x))))))$</p>\n","\n","For the MLP, there are two Linear layers $\\text{Linear}_{1}$ and $\\text{Linear}_{2}$:\n","- $\\text{Linear}_{1}$: input shape `q_features`, output shape `hidden_size`\n","- $\\text{Linear}_{2}$: input shape `hidden_size`, output shape `q_features`\n","\n","Once finished, run the following test cases."]},{"cell_type":"code","execution_count":null,"id":"59e0fd87","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59e0fd87","executionInfo":{"status":"ok","timestamp":1731707677676,"user_tz":300,"elapsed":6552,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"887083c0-6f13-4d9f-f021-6a2693992888"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/10714/hw4_extra\n","plugins: typeguard-4.4.1, anyio-3.7.1\n","collected 112 items / 80 deselected / 32 selected                                                  \u001b[0m\n","\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.0-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-False-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-5-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cpu-0.1-True-64-32-8-27-11-4] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-False-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.0-True-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-False-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-5-4] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_layer[cuda-0.1-True-64-32-8-27-11-4] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","============================================= FAILURES =============================================\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.0-False-64-32-8-27-5-2] _______________________\u001b[0m\n","\n","batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = False, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 2\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f278c880>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f278ca90>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (2, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f278d870>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (2, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.0-False-64-32-8-27-5-4] _______________________\u001b[0m\n","\n","batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = False, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 5\n","x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f284f970>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f284cbe0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (4, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f284f190>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (4, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.0-False-64-32-8-27-11-2] ______________________\u001b[0m\n","\n","batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = False, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 2\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 11\n","x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f2863cd0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f2863c10>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (2, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f2863d30>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (2, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.0-False-64-32-8-27-11-4] ______________________\u001b[0m\n","\n","batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = False, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f27878b0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f2785210>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (4, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f27867a0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (4, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_______________________ test_transformer_layer[cuda-0.0-True-64-32-8-27-5-2] _______________________\u001b[0m\n","\n","batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = True, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 2\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f278c6a0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f278d930>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (2, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f278c850>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (2, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_______________________ test_transformer_layer[cuda-0.0-True-64-32-8-27-5-4] _______________________\u001b[0m\n","\n","batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = True, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 5\n","x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d158cbbad40>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d158cbba980>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (4, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d158cbba860>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (4, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.0-True-64-32-8-27-11-2] _______________________\u001b[0m\n","\n","batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = True, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 2\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 11\n","x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f278cc40>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f278d510>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (2, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f278d4e0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (2, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.0-True-64-32-8-27-11-4] _______________________\u001b[0m\n","\n","batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = True, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f2786230>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f2786560>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (4, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f27867d0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (4, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.1-False-64-32-8-27-5-2] _______________________\u001b[0m\n","\n","batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = False, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 2\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f25f2ad0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f25f2a40>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (2, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f25f2e00>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (2, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.1-False-64-32-8-27-5-4] _______________________\u001b[0m\n","\n","batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = False, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 5\n","x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f284cbe0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f284e4a0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (4, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f284e080>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (4, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.1-False-64-32-8-27-11-2] ______________________\u001b[0m\n","\n","batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = False, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 2\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 11\n","x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f25f1ab0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f25f3610>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (2, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f25f0940>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (2, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.1-False-64-32-8-27-11-4] ______________________\u001b[0m\n","\n","batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = False, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f25d3a90>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f25d3460>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (4, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f25d3250>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (4, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_______________________ test_transformer_layer[cuda-0.1-True-64-32-8-27-5-2] _______________________\u001b[0m\n","\n","batch_size = 2, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = True, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 2\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f28604c0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , -0.7593835 ,\n","         -0.6427555 , -1.0407752 ,  0.9083351 ,  0.429267...   0.32713595, -1.6861233 ,  0.603675  ,  1.5231946 ,\n","          0.31856778,  0.08265579, -0.7877033 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f2862620>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (2, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f2862b90>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (2, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_______________________ test_transformer_layer[cuda-0.1-True-64-32-8-27-5-4] _______________________\u001b[0m\n","\n","batch_size = 4, seq_len = 5, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = True, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 5\n","x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f25d2a70>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00...,  5.16363382e-01, -4.79716718e-01,\n","          7.03656971e-01,  1.89074099e-01, -1.07197821e+00]]],\n","      dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f25d2b60>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (4, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f25d1180>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (4, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.1-True-64-32-8-27-11-2] _______________________\u001b[0m\n","\n","batch_size = 2, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = True, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 2\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 11\n","x          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f2862680>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-8.04154854e-03,  6.31518424e-01,  2.25275445e+00,\n","         -7.59383500e-01, -6.42755508e-01, -1.04077518e+00..., -7.85944700e-01,  2.83783585e-01,\n","          6.31931484e-01,  3.06893229e-01, -1.25974321e+00]]],\n","      dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f2863010>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (2, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f2862470>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (2, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_layer[cuda-0.1-True-64-32-8-27-11-4] _______________________\u001b[0m\n","\n","batch_size = 4, seq_len = 11, input_dim = 27, num_head = 8, dim_head = 32, hidden_size = 64\n","causal = True, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_layer\u001b[39;49;00m(batch_size, seq_len, input_dim, num_head, dim_head, hidden_size, causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 4\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:132: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x7d14f2854730>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 3.4161992 ,  0.04355975, -0.76129997, ...,  0.59368753,\n","          0.31795904,  0.14328356]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x7d14f2857a60>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (4, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7d14f2857cd0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (4, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-False-64-32-8-27-5-2]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-False-64-32-8-27-5-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-False-64-32-8-27-11-2]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-False-64-32-8-27-11-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-True-64-32-8-27-5-2]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-True-64-32-8-27-5-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-True-64-32-8-27-11-2]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.0-True-64-32-8-27-11-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-False-64-32-8-27-5-2]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-False-64-32-8-27-5-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-False-64-32-8-27-11-2]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-False-64-32-8-27-11-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-True-64-32-8-27-5-2]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-True-64-32-8-27-5-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-True-64-32-8-27-11-2]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_layer[cuda-0.1-True-64-32-8-27-11-4]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31m=========================== \u001b[31m\u001b[1m16 failed\u001b[0m, \u001b[32m16 passed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 5.21s\u001b[0m\u001b[31m ===========================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"transformer_layer\""]},{"cell_type":"code","execution_count":null,"id":"b74a6ecb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b74a6ecb","executionInfo":{"status":"ok","timestamp":1731709155144,"user_tz":300,"elapsed":74776,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"90b8ed3b-e004-4fae-b2d9-2e6a68bc1d92"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0\n","rootdir: /content/drive/MyDrive/10714/hw4_extra\n","plugins: typeguard-4.4.1, anyio-3.7.1\n","collecting 0 items                                                                                 \u001b[0mUsing needle backend\n","collected 4 items / 3 deselected / 1 selected                                                      \u001b[0m\n","\n","tests/hw4_extra/test_transformer.py \n","Submitting transformer_layer...\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 1 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 2 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 3 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 4 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 5 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 6 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 7 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 8 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 9 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 10 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 11 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 12 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 13 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 14 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 15 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 16 passed\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Grader test 17 passed\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Grader test 18 passed\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Grader test 19 passed\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Grader test 20 passed\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Grader test 21 passed\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Grader test 22 passed\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Grader test 23 passed\n","(8, 8, 5, 5) (8, 8, 5, 32)\n","Grader test 24 passed\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Grader test 25 passed\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Grader test 26 passed\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Grader test 27 passed\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Grader test 28 passed\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Grader test 29 passed\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Grader test 30 passed\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Grader test 31 passed\n","(8, 8, 11, 11) (8, 8, 11, 32)\n","Grader test 32 passed\n","\u001b[32m.\u001b[0m\n","\n","\u001b[32m============================ \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m3 deselected\u001b[0m\u001b[32m in 69.50s (0:01:09)\u001b[0m\u001b[32m ============================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"cpXlcDbbqL5zz45diS6Z\" -k \"transformer_layer\""]},{"cell_type":"markdown","id":"e0e78953","metadata":{"id":"e0e78953"},"source":["## Part 4 Implementing the Transformer model\n","\n","In this subsection, you will compose the residual transformer layers you implemented in the previous part to build the full Transformer model. Fill in the code in the `Transformer` class by defining a set of `num_layers` `TransformerLayer` modules with the appropriat parameters passed in from the parent `Transformer` class. Then, implement the `self.forward` call of the `Transformer`.\n","\n","As is, your current Transformer layers are permutation-invariant, and cannot tell which position each token is in the sequence. To break this symmetry, you will add a positional embedding to your Transformer.\n","\n","The original Transformer paper uses sinusoidal positional embeddings, and then adds to the input embeddings before the first `TransformerLayer`. These work well, but a more common strategy in modern Transformers is to learn the positional embeddings.\n","\n","To do this, you should use `needle.nn.Embedding`. In your Transformer implementation, create a learnable positional encoding using `needle.nn.Embedding` from homework 4, with `num_embeddings` set as `sequence_len`. Given an input sequence, you should create a tensor that has the timestep id of each token in the sequence (timesteps have increasing value, representing the position of a token in time), and use it like a word id.\n","\n","Last, add the created positional encoding to the input token embeddings before your transformer layers.\n","\n","Once complete, submit the following test cases."]},{"cell_type":"code","execution_count":null,"id":"ec5fb0a7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ec5fb0a7","executionInfo":{"status":"ok","timestamp":1731708972897,"user_tz":300,"elapsed":9286,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"9741d4db-0c99-45ba-9cc4-b00d75e5c105"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/10714/hw4_extra\n","plugins: typeguard-4.4.1, anyio-3.7.1\n","collected 112 items / 80 deselected / 32 selected                                                  \u001b[0m\n","\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  6%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-False-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 21%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.0-True-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 31%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 34%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-False-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-2-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 43%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-5-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 46%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cpu-0.1-True-32-8-4-64-27-11-8] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 56%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 68%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 81%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 87%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 93%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8] \u001b[31mFAILED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n","tests/hw4_extra/test_transformer.py::test_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","============================================= FAILURES =============================================\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2, num_head = 8\n","dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 2\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6649ab340>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6649a9ba0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d6649a8a60>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8] _____________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2, num_head = 8\n","dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 2\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6647a94e0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6647ab760>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d6647a9b70>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4, num_head = 8\n","dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 4\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d664a73580>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d664a710f0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d664a71c30>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8] _____________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4, num_head = 8\n","dim_head = 32, causal = False, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 4\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d664794700>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6647940a0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d664794f70>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2, num_head = 8\n","dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 2\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d664a78670>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d664a7b430>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d664a7bdf0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2, num_head = 8\n","dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 2\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6647e9d80>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6647e8eb0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d6647e9a80>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4, num_head = 8\n","dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 4\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d66478de40>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d66478e5f0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d66478c880>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4, num_head = 8\n","dim_head = 32, causal = True, dropout = 0.0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.0\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 4\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6647ea230>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6647e8df0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d6647eac50>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2, num_head = 8\n","dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 2\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6648789d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d664879990>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d66487b790>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8] _____________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2, num_head = 8\n","dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 2\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6647e87f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6647e87c0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d6647e8310>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4, num_head = 8\n","dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 4\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6649a1ff0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6649a12d0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d6649a1c90>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8] _____________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4, num_head = 8\n","dim_head = 32, causal = False, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = False\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 4\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6649a9420>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6649a9750>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d6649a85e0>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 2, num_head = 8\n","dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 2\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d664a704c0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d664a71000>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d664a71450>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 2, num_head = 8\n","dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 2\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d66479c130>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d66479f460>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d66479d960>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m______________________ test_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 5, input_dim = 27, hidden_size = 64, num_layers = 4, num_head = 8\n","dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 4\n","seq_len    = 5\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6648136d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.6823758 , -0.6566193 ,  0.32996404, ...,  1.2692264 ,\n","          1.2295368 , -0.991682  ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6648106a0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 5, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d664810220>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 5, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[31m\u001b[1m_____________________ test_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8] ______________________\u001b[0m\n","\n","batch_size = 8, seq_len = 11, input_dim = 27, hidden_size = 64, num_layers = 4, num_head = 8\n","dim_head = 32, causal = True, dropout = 0.1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbatch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mseq_len\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m5\u001b[39;49;00m, \u001b[94m11\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_dim\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m27\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mhidden_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m64\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_layers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m8\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdim_head\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m32\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mcausal\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mFalse\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdropout\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0.0\u001b[39;49;00m, \u001b[94m0.1\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_transformer_model\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim,\u001b[90m\u001b[39;49;00m\n","            hidden_size, num_layers,\u001b[90m\u001b[39;49;00m\n","            num_head, dim_head,\u001b[90m\u001b[39;49;00m\n","            causal, dropout, device):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m19943\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        x = np.random.randn(\u001b[90m\u001b[39;49;00m\n","            batch_size, seq_len, input_dim\u001b[90m\u001b[39;49;00m\n","        ).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",">       ndl_x = ndl.Tensor(x, device=device)\u001b[90m\u001b[39;49;00m\n","\n","batch_size = 8\n","causal     = True\n","device     = cuda()\n","dim_head   = 32\n","dropout    = 0.1\n","hidden_size = 64\n","input_dim  = 27\n","num_head   = 8\n","num_layers = 4\n","seq_len    = 11\n","x          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\n","\u001b[1m\u001b[31mtests/hw4_extra/test_transformer.py\u001b[0m:180: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n","    \u001b[0mcached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        array      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = None\n","        kwargs     = {}\n","        requires_grad = True\n","        self       = <[AttributeError(\"'Tensor' object has no attribute 'cached_data'\") raised in repr()] Tensor object at 0x78d6647eb970>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m array_api.array(numpy_array, device=device, dtype=dtype)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        dtype      = None\n","        numpy_array = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:666: in array\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m NDArray(a, device=device)\u001b[90m\u001b[39;49;00m\n","        a          = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        device     = cuda()\n","        dtype      = 'float32'\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:110: in __init__\n","    \u001b[0marray = \u001b[96mself\u001b[39;49;00m.make(other.shape, device=device)\u001b[90m\u001b[39;49;00m\n","        device     = cuda()\n","        other      = array([[[-0.00804155,  0.6315184 ,  2.2527544 , ...,  0.57792866,\n","          1.2252008 , -1.2465482 ],\n","        [ 1.8416...\n","        [ 0.07931643,  0.34981483,  1.627947  , ...,  0.13004941,\n","          1.3878478 ,  0.7909228 ]]], dtype=float32)\n","        self       = <[AttributeError(\"'NDArray' object has no attribute '_device'\") raised in repr()] NDArray object at 0x78d6647e99f0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","shape = (8, 11, 27), strides = None, device = cuda(), handle = None, offset = 0\n","\n","    \u001b[0m\u001b[37m@staticmethod\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m \u001b[92mmake\u001b[39;49;00m(shape, strides=\u001b[94mNone\u001b[39;49;00m, device=\u001b[94mNone\u001b[39;49;00m, handle=\u001b[94mNone\u001b[39;49;00m, offset=\u001b[94m0\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[39;49;00m\n","    \u001b[33m    memory if handle=None, otherwise it will use the handle of an existing\u001b[39;49;00m\n","    \u001b[33m    array.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        array = NDArray.\u001b[92m__new__\u001b[39;49;00m(NDArray)\u001b[90m\u001b[39;49;00m\n","        array._shape = \u001b[96mtuple\u001b[39;49;00m(shape)\u001b[90m\u001b[39;49;00m\n","        array._strides = NDArray.compact_strides(shape) \u001b[94mif\u001b[39;49;00m strides \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m strides\u001b[90m\u001b[39;49;00m\n","        array._offset = offset\u001b[90m\u001b[39;49;00m\n","        array._device = device \u001b[94mif\u001b[39;49;00m device \u001b[95mis\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m default_device()\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m handle \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",">           array._handle = array.device.Array(prod(shape))\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           RuntimeError: CUDA driver version is insufficient for CUDA runtime version\u001b[0m\n","\n","array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x78d6647eb250>\n","device     = cuda()\n","handle     = None\n","offset     = 0\n","shape      = (8, 11, 27)\n","strides    = None\n","\n","\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:146: RuntimeError\n","\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-2-64-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-2-64-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-4-64-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-False-32-8-4-64-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-2-64-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-2-64-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-4-64-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.0-True-32-8-4-64-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-2-64-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-2-64-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-4-64-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-False-32-8-4-64-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-2-64-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-2-64-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-4-64-27-5-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31mFAILED\u001b[0m tests/hw4_extra/test_transformer.py::\u001b[1mtest_transformer_model[cuda-0.1-True-32-8-4-64-27-11-8]\u001b[0m - RuntimeError: CUDA driver version is insufficient for CUDA runtime version\n","\u001b[31m=========================== \u001b[31m\u001b[1m16 failed\u001b[0m, \u001b[32m16 passed\u001b[0m, \u001b[33m80 deselected\u001b[0m\u001b[31m in 7.67s\u001b[0m\u001b[31m ===========================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"transformer_model\""]},{"cell_type":"code","execution_count":null,"id":"4c897377","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4c897377","executionInfo":{"status":"ok","timestamp":1731709206200,"user_tz":300,"elapsed":36594,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"a8156673-77e0-44fd-eeec-366130795692"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0\n","rootdir: /content/drive/MyDrive/10714/hw4_extra\n","plugins: typeguard-4.4.1, anyio-3.7.1\n","\u001b[1mcollecting ... \u001b[0mUsing needle backend\n","collected 4 items / 3 deselected / 1 selected                                                      \u001b[0m\n","\n","tests/hw4_extra/test_transformer.py \n","Submitting transformer_model...\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 1 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 2 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 3 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 4 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 5 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 6 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 7 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 8 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 9 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 10 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 11 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 12 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 13 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 14 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 15 passed\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","(4, 8, 5, 5) (4, 8, 5, 32)\n","Grader test 16 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 17 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 18 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 19 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 20 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 21 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 22 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 23 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 24 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 25 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 26 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 27 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 28 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 29 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 30 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 31 passed\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","(4, 8, 11, 11) (4, 8, 11, 32)\n","Grader test 32 passed\n","\u001b[32m.\u001b[0m\n","\n","\u001b[32m================================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m3 deselected\u001b[0m\u001b[32m in 35.45s\u001b[0m\u001b[32m =================================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"cpXlcDbbqL5zz45diS6Z\" -k \"transformer_model\""]},{"cell_type":"markdown","id":"899683fc","metadata":{"id":"899683fc"},"source":["Now, you can train a Transformer language model on the Penn Treebank dataset:\n","\n","Note: make sure to initialize a transformer model in the class `LanguageModel` of `apps/models.py`; also for Transformers, the final linear head `self.linear` should take in input dimension `embedding_size`."]},{"cell_type":"code","execution_count":null,"id":"d118e5db","metadata":{"id":"d118e5db"},"outputs":[],"source":["import needle as ndl\n","sys.path.append('./apps')\n","from models import LanguageModel\n","from simple_ml import train_ptb, evaluate_ptb\n","\n","device = ndl.cuda()\n","corpus = ndl.data.Corpus(\"data/ptb\")\n","train_data = ndl.data.batchify(corpus.train, batch_size=256, device=device, dtype=\"float32\")\n","model = LanguageModel(20, len(corpus.dictionary), hidden_size=32, num_layers=1, seq_model='transformer', seq_len=20, device=device)\n","train_ptb(model, train_data, seq_len=20, n_epochs=10, device=device, lr=0.003, optimizer=ndl.optim.Adam)\n","evaluate_ptb(model, train_data, seq_len=20, device=device)"]},{"cell_type":"code","source":["# !tar -czf my_submission.tar.gz src/ python/ apps/ # remove non-existing directories"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7-IC9fL46pXm","executionInfo":{"status":"ok","timestamp":1731709315400,"user_tz":300,"elapsed":22967,"user":{"displayName":"Lijie Yang","userId":"13115200329431449250"}},"outputId":"ca01be91-323f-4b1b-bfb5-5a5c2a9cce68"},"id":"7-IC9fL46pXm","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tar: src/ndarray_backend_cpu.cc: file changed as we read it\n","tar: src/ndarray_backend_cuda.cu: file changed as we read it\n","tar: python/needle/__init__.py: file changed as we read it\n","tar: python/needle/autograd.py: file changed as we read it\n","tar: python/needle/backend_ndarray/__init__.py: file changed as we read it\n","tar: python/needle/backend_ndarray/ndarray_backend_numpy.py: file changed as we read it\n","tar: python/needle/backend_ndarray/__pycache__/__init__.cpython-310.pyc: file changed as we read it\n","tar: python/needle/backend_ndarray/__pycache__/ndarray_backend_numpy.cpython-310.pyc: file changed as we read it\n","tar: python/needle/backend_ndarray/__pycache__/ndarray.cpython-310.pyc: file changed as we read it\n","tar: python/needle/backend_ndarray/ndarray.py: file changed as we read it\n","tar: python/needle/backend_ndarray/ndarray_backend_cuda.cpython-310-x86_64-linux-gnu.so: file changed as we read it\n","tar: python/needle/backend_ndarray/ndarray_backend_cpu.cpython-310-x86_64-linux-gnu.so: file changed as we read it\n","tar: python/needle/backend_selection.py: file changed as we read it\n","tar: python/needle/data/__init__.py: file changed as we read it\n","tar: python/needle/data/data_basic.py: file changed as we read it\n","tar: python/needle/data/data_transforms.py: file changed as we read it\n","tar: python/needle/data/datasets/__init__.py: file changed as we read it\n","tar: python/needle/data/datasets/cifar10_dataset.py: file changed as we read it\n","tar: python/needle/data/datasets/mnist_dataset.py: file changed as we read it\n","tar: python/needle/data/datasets/ndarray_dataset.py: file changed as we read it\n","tar: python/needle/data/datasets/ptb_dataset.py: file changed as we read it\n","tar: python/needle/data/datasets/__pycache__/__init__.cpython-310.pyc: file changed as we read it\n","tar: python/needle/data/datasets/__pycache__/mnist_dataset.cpython-310.pyc: file changed as we read it\n","tar: python/needle/data/datasets/__pycache__/ndarray_dataset.cpython-310.pyc: file changed as we read it\n","tar: python/needle/data/datasets/__pycache__/cifar10_dataset.cpython-310.pyc: file changed as we read it\n","tar: python/needle/data/datasets/__pycache__/ptb_dataset.cpython-310.pyc: file changed as we read it\n","tar: python/needle/data/__pycache__/__init__.cpython-310.pyc: file changed as we read it\n","tar: python/needle/data/__pycache__/data_basic.cpython-310.pyc: file changed as we read it\n","tar: python/needle/data/__pycache__/data_transforms.cpython-310.pyc: file changed as we read it\n","tar: python/needle/init/__init__.py: file changed as we read it\n","tar: python/needle/init/__pycache__/__init__.cpython-310.pyc: file changed as we read it\n","tar: python/needle/init/__pycache__/init_basic.cpython-310.pyc: file changed as we read it\n","tar: python/needle/init/__pycache__/init_initializers.cpython-310.pyc: file changed as we read it\n","tar: python/needle/init/init_basic.py: file changed as we read it\n","tar: python/needle/init/init_initializers.py: file changed as we read it\n","tar: python/needle/nn/__init__.py: file changed as we read it\n","tar: python/needle/nn/nn_conv.py: file changed as we read it\n","tar: python/needle/nn/__pycache__/__init__.cpython-310.pyc: file changed as we read it\n","tar: python/needle/nn/__pycache__/nn_conv.cpython-310.pyc: file changed as we read it\n","tar: python/needle/nn/__pycache__/nn_basic.cpython-310.pyc: file changed as we read it\n","tar: python/needle/nn/__pycache__/nn_sequence.cpython-310.pyc: file changed as we read it\n","tar: python/needle/nn/__pycache__/nn_transformer.cpython-310.pyc: file changed as we read it\n","tar: python/needle/nn/nn_sequence.py: file changed as we read it\n","tar: python/needle/nn/nn_transformer.py: file changed as we read it\n","tar: python/needle/ops/__init__.py: file changed as we read it\n","tar: python/needle/ops/ops_tuple.py: file changed as we read it\n","tar: python/needle/ops/ops_logarithmic.py: file changed as we read it\n","tar: python/needle/ops/__pycache__/__init__.cpython-310.pyc: file changed as we read it\n","tar: python/needle/ops/__pycache__/ops_tuple.cpython-310.pyc: file changed as we read it\n","tar: python/needle/ops/__pycache__/ops_mathematic.cpython-310.pyc: file changed as we read it\n","tar: python/needle/ops/__pycache__/ops_logarithmic.cpython-310.pyc: file changed as we read it\n","tar: python/needle/ops/ops_mathematic.py: file changed as we read it\n","tar: python/needle/optim.py: file changed as we read it\n","tar: python/needle/__pycache__/__init__.cpython-310.pyc: file changed as we read it\n","tar: python/needle/__pycache__/autograd.cpython-310.pyc: file changed as we read it\n","tar: python/needle/__pycache__/backend_selection.cpython-310.pyc: file changed as we read it\n","tar: python/needle/__pycache__/backend_numpy.cpython-310.pyc: file changed as we read it\n","tar: python/needle/backend_numpy.py: file changed as we read it\n","tar: apps/models.py: file changed as we read it\n","tar: apps/simple_ml.py: file changed as we read it\n","tar: apps/__pycache__/simple_ml.cpython-310.pyc: file changed as we read it\n","tar: apps/__pycache__/models.cpython-310.pyc: file changed as we read it\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}